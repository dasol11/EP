{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe34a7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import math\n",
    "import random\n",
    "import timeit\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8a00cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim, hid1_dim, hid2_dim, out_dim = 4, 16, 8, 3\n",
    "biases = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13db6582",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris['data']\n",
    "y = iris['target']\n",
    "names = iris['target_names']\n",
    "feature_names = iris['feature_names']\n",
    "\n",
    "# Scale data to have mean 0 and variance 1 \n",
    "# which is importance for convergence of the neural network\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data set into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=2)\n",
    "\n",
    "X_train = Variable(torch.from_numpy(X_train), requires_grad = False).float()\n",
    "y_train = Variable(torch.from_numpy(y_train), requires_grad = False).long()\n",
    "X_test  = Variable(torch.from_numpy(X_test), requires_grad = False).float()\n",
    "y_test  = Variable(torch.from_numpy(y_test), requires_grad = False).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "284a1964",
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = torch.nn.ReLU()\n",
    "soft = torch.nn.Softmax(dim=1)\n",
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17322814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fitness(X_train, y_train, weights):\n",
    "    w1, w2, w3 = [], [], []\n",
    "    lengths = 0\n",
    "    for i in range(in_dim) :\n",
    "        w1.append(weights[lengths : hid1_dim + lengths])\n",
    "        lengths += hid1_dim\n",
    "    w1 = np.array(w1)\n",
    "    \n",
    "    for i in range(hid1_dim) :\n",
    "        w2.append(weights[lengths : hid2_dim + lengths])\n",
    "        lengths += hid2_dim\n",
    "    w2 = np.array(w2)\n",
    "    \n",
    "    for i in range(hid2_dim) :\n",
    "        w3.append(weights[lengths : out_dim + lengths])\n",
    "        lengths += out_dim\n",
    "    w3 = np.array(w3)\n",
    "    \n",
    "    b = weights[lengths :]\n",
    "\n",
    "    w1, w2, w3, b = Variable(torch.from_numpy(w1)).float(), \\\n",
    "    Variable(torch.from_numpy(w2)).float(), \\\n",
    "    Variable(torch.from_numpy(w3)).float(), Variable(torch.from_numpy(b)).float()\n",
    "    \n",
    "    y_pred = X_train.mm(w1)\n",
    "    y_pred = y_pred + b[0]\n",
    "    y_pred = relu(y_pred.clone().detach())\n",
    "    y_pred = y_pred.mm(w2)\n",
    "    y_pred = y_pred + b[1]\n",
    "    y_pred = relu(y_pred.clone().detach())\n",
    "    y_pred = y_pred.mm(w3)\n",
    "    y_pred = y_pred + b[2]\n",
    "    y_pred = soft(y_pred.clone().detach())\n",
    "    loss = loss_func(y_pred, y_train)\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "def tester(X_test, y_test, weights):\n",
    "    w1, w2, w3 = [], [], []\n",
    "    lengths = 0\n",
    "    for i in range(in_dim) :\n",
    "        w1.append(weights[lengths : hid1_dim + lengths])\n",
    "        lengths += hid1_dim\n",
    "    w1 = np.array(w1)\n",
    "    \n",
    "    for i in range(hid1_dim) :\n",
    "        w2.append(weights[lengths : hid2_dim + lengths])\n",
    "        lengths += hid2_dim\n",
    "    w2 = np.array(w2)\n",
    "    \n",
    "    for i in range(hid2_dim) :\n",
    "        w3.append(weights[lengths : out_dim + lengths])\n",
    "        lengths += out_dim\n",
    "    w3 = np.array(w3)\n",
    "\n",
    "    b = weights[lengths :]\n",
    "    \n",
    "    w1, w2, w3, b = Variable(torch.from_numpy(w1)).float(), Variable(torch.from_numpy(w2)).float(),\\\n",
    "    Variable(torch.from_numpy(w3)).float(), Variable(torch.from_numpy(b)).float()\n",
    "    \n",
    "    y_pred = X_test.mm(w1)\n",
    "    y_pred = y_pred + b[0]\n",
    "    y_pred = relu(y_pred.clone().detach())\n",
    "    y_pred = y_pred.mm(w2)\n",
    "    y_pred = y_pred + b[1]\n",
    "    y_pred = relu(y_pred.clone().detach())\n",
    "    y_pred = y_pred.mm(w3)\n",
    "    y_pred = y_pred + b[2]\n",
    "    y_pred = soft(y_pred.clone().detach())\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8be5b51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment, EPOCHS = 20, 500\n",
    "train_loss_list = np.zeros((experiment,))\n",
    "val_loss_list = np.zeros((experiment,))\n",
    "val_acc_list = np.zeros((experiment,))\n",
    "\n",
    "memory_val_loss_list = np.full((experiment,), float(\"inf\"))\n",
    "memory_train_loss_list = np.zeros((experiment,))\n",
    "memory_val_acc_list = np.zeros((experiment,))\n",
    "\n",
    "lb, ub, dim = -1, 1, in_dim * hid1_dim + hid1_dim * hid2_dim + hid2_dim * out_dim + biases\n",
    "PHASE = 0.1\n",
    "PSO_EPOCHS = int(EPOCHS * PHASE)\n",
    "GWO_EPOCHS = EPOCHS - PSO_EPOCHS\n",
    "swarm_no = 60\n",
    "wolves_no = 20\n",
    "\n",
    "inertia_w = 0.3 # inertia constant\n",
    "c1 = 1 # cognitive constant\n",
    "c2 = 1 # social constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da7fa077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  Train_loss : 0.5613499283790588 Val_loss : 0.5683485865592957 Val_acc : 0.9666666388511658\n",
      "1  Train_loss : 0.616433322429657 Val_loss : 0.6377427577972412 Val_acc : 0.8666666746139526\n",
      "2  Train_loss : 0.5598674416542053 Val_loss : 0.6262403726577759 Val_acc : 0.9333333373069763\n",
      "3  Train_loss : 0.5599876046180725 Val_loss : 0.6073557734489441 Val_acc : 0.9333333373069763\n",
      "4  Train_loss : 0.5597978830337524 Val_loss : 0.5518413782119751 Val_acc : 1.0\n",
      "5  Train_loss : 0.5681113004684448 Val_loss : 0.5847779512405396 Val_acc : 0.9666666388511658\n",
      "6  Train_loss : 0.5712512135505676 Val_loss : 0.6210106015205383 Val_acc : 0.9333333373069763\n",
      "7  Train_loss : 0.5616281628608704 Val_loss : 0.6089903712272644 Val_acc : 0.9333333373069763\n",
      "8  Train_loss : 0.5597780346870422 Val_loss : 0.5753638744354248 Val_acc : 0.9666666388511658\n",
      "9  Train_loss : 0.5597782731056213 Val_loss : 0.6151220798492432 Val_acc : 0.9333333373069763\n",
      "10  Train_loss : 0.5597789287567139 Val_loss : 0.6072059869766235 Val_acc : 0.9333333373069763\n",
      "11  Train_loss : 0.5598032474517822 Val_loss : 0.5847774147987366 Val_acc : 0.9666666388511658\n",
      "12  Train_loss : 0.5597781538963318 Val_loss : 0.5520595908164978 Val_acc : 1.0\n",
      "13  Train_loss : 0.5681117177009583 Val_loss : 0.5859434604644775 Val_acc : 0.9666666388511658\n",
      "14  Train_loss : 0.5681451559066772 Val_loss : 0.6191294193267822 Val_acc : 0.9333333373069763\n",
      "15  Train_loss : 0.5600622296333313 Val_loss : 0.5835981369018555 Val_acc : 0.9666666388511658\n",
      "16  Train_loss : 0.5597802996635437 Val_loss : 0.6012100577354431 Val_acc : 0.9333333373069763\n",
      "17  Train_loss : 0.5597808361053467 Val_loss : 0.5838449001312256 Val_acc : 0.9666666388511658\n",
      "18  Train_loss : 0.5515538454055786 Val_loss : 0.626241147518158 Val_acc : 0.9333333373069763\n",
      "19  Train_loss : 0.5597782135009766 Val_loss : 0.5514823794364929 Val_acc : 1.0\n"
     ]
    }
   ],
   "source": [
    "experiment_date = datetime.datetime.now()\n",
    "starter = timeit.default_timer()\n",
    "for exper in range(20) :\n",
    "    population = np.zeros((swarm_no, dim))\n",
    "    velocity = np.ones((swarm_no, dim))\n",
    "    particle_best_pos = np.zeros((swarm_no, dim))\n",
    "    particle_best_sco = np.zeros((swarm_no))\n",
    "    swarm_best_pos = np.zeros(dim)\n",
    "    swarm_best_sco = float(\"inf\")\n",
    "\n",
    "    # PSO PHASE\n",
    "    for i in range(swarm_no) :\n",
    "        population[i, :] = np.random.uniform(lb, ub, dim)\n",
    "        velocity[i, :] = np.random.uniform(lb, ub, dim)\n",
    "        init_fit = get_fitness(X_train, y_train, population[i, :])\n",
    "\n",
    "        particle_best_pos[i, :] = population[i, :]\n",
    "        particle_best_sco[i] = init_fit \n",
    "\n",
    "        if swarm_best_sco > init_fit :\n",
    "            swarm_best_pos = population[i, :]\n",
    "            swarm_best_sco = init_fit\n",
    "            \n",
    "    for epoch in range(PSO_EPOCHS) :\n",
    "        for i in range(swarm_no) :\n",
    "            r1, r2 = np.random.rand(dim), np.random.rand(dim)\n",
    "            velo_cog = c1 * r1 * (particle_best_pos[i, :].copy() - population[i, :].copy())\n",
    "            velo_soc = c2 * r2 * (swarm_best_pos.copy() - population[i, :].copy())\n",
    "            velocity[i, :] = inertia_w * velocity[i, :].copy() + velo_cog.copy() + velo_soc.copy()\n",
    "            population[i, :] = population[i, :].copy() + velocity[i, :].copy()\n",
    "            result_pos = population[i, :].copy()\n",
    "            result_fit = get_fitness(X_train, y_train, result_pos)\n",
    "            \n",
    "            # update particle_best\n",
    "            if particle_best_sco[i] > result_fit :\n",
    "                particle_best_pos[i] = result_pos\n",
    "                particle_best_sco[i] = result_fit\n",
    "\n",
    "            # update swarm_best\n",
    "            if swarm_best_sco > result_fit :\n",
    "                swarm_best_pos = result_pos\n",
    "                swarm_best_sco = result_fit\n",
    "\n",
    "        w_set = swarm_best_pos\n",
    "        valid = tester(X_test, y_test, w_set)\n",
    "        val_loss = loss_func(valid, y_test).item()\n",
    "\n",
    "        if val_loss < memory_val_loss_list[exper] :\n",
    "            memory_val_loss_list[exper] = val_loss\n",
    "            memory_train_loss_list[exper] = swarm_best_sco\n",
    "            \n",
    "            val_correct = (torch.argmax(valid, dim=1) == y_test).type(torch.FloatTensor)\n",
    "            memory_val_acc_list[exper] = val_correct.mean().item()\n",
    "                \n",
    "    # GWO Phase\n",
    "    alpha_pos = swarm_best_pos\n",
    "    alpha_score = swarm_best_sco\n",
    "\n",
    "    beta_pos = np.zeros(dim)\n",
    "    beta_score = float(\"inf\")\n",
    "\n",
    "    delta_pos = np.zeros(dim)\n",
    "    delta_score = float(\"inf\")\n",
    "    \n",
    "    sampler = random.sample(range(swarm_no), wolves_no)\n",
    "    new_population = np.zeros((wolves_no, dim))\n",
    "\n",
    "    for i in range(wolves_no) :\n",
    "        new_population[i, :] = population[sampler[i], :] \n",
    "        fitness = get_fitness(X_train, y_train, population[i, :])\n",
    "\n",
    "        if fitness < alpha_score:\n",
    "            delta_score = beta_score  # Update delta\n",
    "            delta_pos = beta_pos.copy()\n",
    "            beta_score = alpha_score  # Update beta\n",
    "            beta_pos = alpha_pos.copy()\n",
    "            alpha_score = fitness  # Update alpha\n",
    "            alpha_pos = new_population[i, :].copy()\n",
    "\n",
    "        if fitness > alpha_score and fitness < beta_score:\n",
    "            delta_score = beta_score  # Update delte\n",
    "            delta_pos = beta_pos.copy()\n",
    "            beta_score = fitness  # Update beta\n",
    "            beta_pos = new_population[i, :].copy()\n",
    "\n",
    "        if fitness > alpha_score and fitness > beta_score and fitness < delta_score:\n",
    "            delta_score = fitness  # Update delta\n",
    "            delta_pos = new_population[i, :].copy()\n",
    "\n",
    "    population = new_population\n",
    "    \n",
    "    for epoch in range(GWO_EPOCHS) :\n",
    "    # a는 선형적으로 감소하는 값으로 2 ~ 0을 가짐\n",
    "        a = 2 - (PSO_EPOCHS + epoch) * ((2) / EPOCHS)\n",
    "    \n",
    "        for i in range(wolves_no) :\n",
    "            r1 = np.random.rand(dim)  # r1 is a random number in [0,1]\n",
    "            r2 = np.random.rand(dim)  # r2 is a random number in [0,1]\n",
    "            A1 = 2 * a * r1 - a  # Equation (3.3)\n",
    "            C1 = 2 * r2  # Equation (3.4)\n",
    "            D_alpha = abs(C1 * alpha_pos - population[i, :])  # Equation (3.5)-part 1\n",
    "            X1 = alpha_pos - A1 * D_alpha  # Equation (3.6)-part 1\n",
    "\n",
    "            r1 = np.random.rand(dim)\n",
    "            r2 = np.random.rand(dim)\n",
    "            A2 = 2 * a * r1 - a  # Equation (3.3)\n",
    "            C2 = 2 * r2  # Equation (3.4)\n",
    "            D_beta = abs(C2 * beta_pos - population[i, :])  # Equation (3.5)-part 2\n",
    "            X2 = beta_pos - A2 * D_beta  # Equation (3.6)-part 2\n",
    "\n",
    "            r1 = np.random.rand(dim)\n",
    "            r2 = np.random.rand(dim)\n",
    "            A3 = 2 * a * r1 - a  # Equation (3.3)\n",
    "            C3 = 2 * r2  # Equation (3.4)\n",
    "            D_delta = abs(C3 * delta_pos - population[i, :])  # Equation (3.5)-part 3\n",
    "            X3 = delta_pos - A3 * D_delta  # Equation (3.5)-part 3\n",
    "\n",
    "            population[i, :] = ((X1 + X2 + X3) / 3)  # Equation (3.7)\n",
    "\n",
    "            fitness = get_fitness(X_train, y_train, population[i, :])\n",
    "\n",
    "            if fitness < alpha_score:\n",
    "                delta_score = beta_score  # Update delta\n",
    "                delta_pos = beta_pos.copy()\n",
    "                beta_score = alpha_score  # Update beta\n",
    "                beta_pos = alpha_pos.copy()\n",
    "                alpha_score = fitness  # Update alpha\n",
    "                alpha_pos = population[i, :].copy()\n",
    "\n",
    "            if fitness > alpha_score and fitness < beta_score:\n",
    "                delta_score = beta_score  # Update delte\n",
    "                delta_pos = beta_pos.copy()\n",
    "                beta_score = fitness  # Update beta\n",
    "                beta_pos = population[i, :].copy()\n",
    "\n",
    "            if fitness > alpha_score and fitness > beta_score and fitness < delta_score:\n",
    "                delta_score = fitness  # Update delta\n",
    "                delta_pos = population[i, :].copy()\n",
    "                \n",
    "        w_set = alpha_pos.copy()\n",
    "        valid = tester(X_test, y_test, w_set)\n",
    "        val_loss = loss_func(valid, y_test).item()\n",
    "\n",
    "        if val_loss < memory_val_loss_list[exper] :\n",
    "            memory_val_loss_list[exper] = val_loss\n",
    "            memory_train_loss_list[exper] = alpha_score\n",
    "            \n",
    "            val_correct = (torch.argmax(valid, dim=1) == y_test).type(torch.FloatTensor)\n",
    "            memory_val_acc_list[exper] = val_correct.mean().item()\n",
    "\n",
    "    train_loss_list[exper] = alpha_score\n",
    "    \n",
    "    w_set = alpha_pos\n",
    "    valid = tester(X_test, y_test, w_set)\n",
    "    val_loss_list[exper] = loss_func(valid, y_test).item()\n",
    "    \n",
    "    val_correct = (torch.argmax(valid, dim=1) == y_test).type(torch.FloatTensor)\n",
    "    val_acc_list[exper] = val_correct.mean().item()\n",
    "    \n",
    "    print(exper, \" Train_loss :\", train_loss_list[exper], \"Val_loss :\", val_loss_list[exper], \"Val_acc :\", val_acc_list[exper])\n",
    "ender = timeit.default_timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cad88847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실험 정보 : KOPT, on IRIS\n",
      "실험 횟수 : 20 \n",
      "실험 일자 : 2022-08-01 11:43:22.836978\n",
      "Computational Time : 90.92704073898494\n",
      "train_loss min : 0.5515538454055786\n",
      "train_loss mean : 0.5642277896404266\n",
      "train_loss std : 0.012704361898614897\n",
      "\n",
      "val_loss min : 0.5514823794364929\n",
      "val_loss mean : 0.5946143120527267\n",
      "val_loss std : 0.025707215389054856\n",
      "\n",
      "val_acc max : 1.0\n",
      "val_acc mean : 0.951666659116745\n",
      "val_acc std : 0.030686580804760433\n",
      "\n",
      "Memory_val min : 0.5514507293701172\n",
      "Memory_val mean : 0.5705717146396637\n",
      "Memory_val std : 0.017191575078465627\n",
      "\n",
      "Memory_train min : 0.55977863073349\n",
      "Memory_train mean : 0.5708203166723251\n",
      "Memory_train std : 0.013288832621265283\n",
      "\n",
      "Memory_acc min : 0.8999999761581421\n",
      "Memory_acc mean : 0.9816666543483734\n",
      "Memory_acc std : 0.024664425025287668\n"
     ]
    }
   ],
   "source": [
    "print(\"실험 정보 : KOPT, on IRIS\")\n",
    "print(\"실험 횟수 :\", experiment, \"\\n\"+\"실험 일자 :\", experiment_date)\n",
    "print(\"Computational Time :\", ender - starter)\n",
    "print(\"train_loss min :\", np.min(train_loss_list))\n",
    "print(\"train_loss mean :\", np.mean(train_loss_list))\n",
    "print(\"train_loss std :\", np.std(train_loss_list))\n",
    "print()\n",
    "print(\"val_loss min :\", np.min(val_loss_list))\n",
    "print(\"val_loss mean :\", np.mean(val_loss_list))\n",
    "print(\"val_loss std :\", np.std(val_loss_list))\n",
    "print()\n",
    "print(\"val_acc max :\", np.max(val_acc_list))\n",
    "print(\"val_acc mean :\", np.mean(val_acc_list))\n",
    "print(\"val_acc std :\", np.std(val_acc_list))\n",
    "print()\n",
    "print(\"Memory_val min :\", np.min(memory_val_loss_list))\n",
    "print(\"Memory_val mean :\", np.mean(memory_val_loss_list))\n",
    "print(\"Memory_val std :\", np.std(memory_val_loss_list))\n",
    "print()\n",
    "print(\"Memory_train min :\", np.min(memory_train_loss_list))\n",
    "print(\"Memory_train mean :\", np.mean(memory_train_loss_list))\n",
    "print(\"Memory_train std :\", np.std(memory_train_loss_list))\n",
    "print()\n",
    "print(\"Memory_acc min :\", np.min(memory_val_acc_list))\n",
    "print(\"Memory_acc mean :\", np.mean(memory_val_acc_list))\n",
    "print(\"Memory_acc std :\", np.std(memory_val_acc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b16ecfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32af27ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba62e37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e413bd75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8f565e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8bb9ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8a99b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea8dd23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816c5fc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd19a5e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c5f39e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69adc34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72e076e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8987ade1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd63dc3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f9ace9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05abf4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7ab772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce9100d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c33202",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15ee343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f71cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
